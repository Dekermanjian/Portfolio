{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "from sparknlp.common import *\n",
    "import pyspark.sql.functions as F\n",
    "from sparknlp.training import CoNLL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start a Spark Session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = sparknlp.start(gpu=False)\n",
    "# sqlContext = SQLContext(spark)\n",
    "# sqlContext.setConf('spark.sql.shuffle.partitions', '10') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in the CoNLL Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Imaging-AMARETTO ...|[{document, 0, 15...|[{document, 0, 15...|[{token, 0, 15, I...|[{pos, 0, 15, NN,...|[{named_entity, 0...|\n",
      "|The availability ...|[{document, 0, 22...|[{document, 0, 22...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|Here , present Im...|[{document, 0, 25...|[{document, 0, 25...|[{token, 0, 3, He...|[{pos, 0, 3, RB, ...|[{named_entity, 0...|\n",
      "|To demonstrate ut...|[{document, 0, 34...|[{document, 0, 34...|[{token, 0, 1, To...|[{pos, 0, 1, TO, ...|[{named_entity, 0...|\n",
      "|Our results show ...|[{document, 0, 20...|[{document, 0, 20...|[{token, 0, 2, Ou...|[{pos, 0, 2, PRP$...|[{named_entity, 0...|\n",
      "|Imaging-AMARETTO ...|[{document, 0, 21...|[{document, 0, 21...|[{token, 0, 15, I...|[{pos, 0, 15, NNP...|[{named_entity, 0...|\n",
      "|Our network-based...|[{document, 0, 17...|[{document, 0, 17...|[{token, 0, 2, Ou...|[{pos, 0, 2, PRP$...|[{named_entity, 0...|\n",
      "|We anticipate Ima...|[{document, 0, 33...|[{document, 0, 33...|[{token, 0, 1, We...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "|In current study ...|[{document, 0, 19...|[{document, 0, 19...|[{token, 0, 1, In...|[{pos, 0, 1, IN, ...|[{named_entity, 0...|\n",
      "|The advancement b...|[{document, 0, 15...|[{document, 0, 15...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|The ex vivo tumor...|[{document, 0, 26...|[{document, 0, 26...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|The spectra pre-p...|[{document, 0, 16...|[{document, 0, 16...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|In present study ...|[{document, 0, 12...|[{document, 0, 12...|[{token, 0, 1, In...|[{pos, 0, 1, IN, ...|[{named_entity, 0...|\n",
      "|The chosen featur...|[{document, 0, 15...|[{document, 0, 15...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|The analysis demo...|[{document, 0, 15...|[{document, 0, 15...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|The serum metabol...|[{document, 0, 24...|[{document, 0, 24...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|\n",
      "|Accurate high-qua...|[{document, 0, 17...|[{document, 0, 17...|[{token, 0, 7, Ac...|[{pos, 0, 7, NNP,...|[{named_entity, 0...|\n",
      "|However , major c...|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 6, Ho...|[{pos, 0, 6, RB, ...|[{named_entity, 0...|\n",
      "|This requires pow...|[{document, 0, 14...|[{document, 0, 14...|[{token, 0, 3, Th...|[{pos, 0, 3, DT, ...|[{named_entity, 0...|\n",
      "|We designed Lipid...|[{document, 0, 54...|[{document, 0, 54...|[{token, 0, 1, We...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training Data\n",
    "train = CoNLL().readDataset(spark, 'Data/trainingTextProcessed.txt')\n",
    "train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = CoNLL().readDataset(spark, 'Data/testingTextProcessed.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elmo download started this may take some time.\n",
      "Approximate size to download 334.1 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# Normalize the text\n",
    "normalizer = (\n",
    "    Normalizer()\n",
    "    .setInputCols(['token'])\n",
    "    .setOutputCol('normalized')\n",
    "    .setLowercase(False)\n",
    "#     .setCleanupPatterns([\"[,\\.\\s]\"])\n",
    "    .setCleanupPatterns([\"[^\\w\\d\\s]\"]) # Removes punctuation\n",
    ")\n",
    "\n",
    "# Get ELMo word embeddings\n",
    "elmo = (\n",
    "    ElmoEmbeddings.pretrained()\n",
    "    .setInputCols(\"sentence\", \"normalized\")\n",
    "    .setOutputCol(\"elmo\")\n",
    ")\n",
    "# Define the NER deep learning model\n",
    "nerTagger = (\n",
    "    NerDLApproach()\n",
    "    .setInputCols([\"sentence\", \"normalized\", \"elmo\"])\n",
    "    .setLabelColumn(\"label\")\n",
    "    .setOutputCol(\"ner\")\n",
    "    .setMaxEpochs(50)\n",
    "    .setLr(0.01) # learning rate\n",
    "    .setPo(0.005) # learning rate decay\n",
    "    .setBatchSize(1024)\n",
    "    .setRandomSeed(777)\n",
    "    .setVerbose(1)\n",
    "    .setValidationSplit(0.1) # Make a validation split\n",
    "    .setEvaluationLogExtended(True)\n",
    "    .setEnableOutputLogs(True)\n",
    "    .setIncludeConfidence(True) # include confidence values\n",
    "    #.setTestDataset('Data/testingTextProcessed_full.parquet')\n",
    "    .setTestDataset('Data/testingTextProcessed.parquet')\n",
    ")\n",
    "# Construct the pipeline\n",
    "ner_pipeline = Pipeline(stages=[normalizer, elmo, nerTagger])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Elmo to Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 pos|               label|          normalized|                elmo|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Pathway Tools ver...|[{document, 0, 88...|[{document, 0, 88...|[{token, 0, 6, Pa...|[{pos, 0, 6, NNP,...|[{named_entity, 0...|[{token, 0, 6, Pa...|[{word_embeddings...|\n",
      "|Biological system...|[{document, 0, 11...|[{document, 0, 11...|[{token, 0, 9, Bi...|[{pos, 0, 9, JJ, ...|[{named_entity, 0...|[{token, 0, 9, Bi...|[{word_embeddings...|\n",
      "|Our development P...|[{document, 0, 20...|[{document, 0, 20...|[{token, 0, 2, Ou...|[{pos, 0, 2, PRP$...|[{named_entity, 0...|[{token, 0, 2, Ou...|[{word_embeddings...|\n",
      "|Further , sought ...|[{document, 0, 15...|[{document, 0, 15...|[{token, 0, 6, Fu...|[{pos, 0, 6, NNP,...|[{named_entity, 0...|[{token, 0, 6, Fu...|[{word_embeddings...|\n",
      "|In past 4 years e...|[{document, 0, 83...|[{document, 0, 83...|[{token, 0, 1, In...|[{pos, 0, 1, IN, ...|[{named_entity, 0...|[{token, 0, 1, In...|[{word_embeddings...|\n",
      "|It support metabo...|[{document, 0, 21...|[{document, 0, 21...|[{token, 0, 1, It...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|[{token, 0, 1, It...|[{word_embeddings...|\n",
      "|Pathway Tools sup...|[{document, 0, 25...|[{document, 0, 25...|[{token, 0, 6, Pa...|[{pos, 0, 6, NNP,...|[{named_entity, 0...|[{token, 0, 6, Pa...|[{word_embeddings...|\n",
      "|We also improved ...|[{document, 0, 23...|[{document, 0, 23...|[{token, 0, 1, We...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|[{token, 0, 1, We...|[{word_embeddings...|\n",
      "|The software free...|[{document, 0, 61...|[{document, 0, 61...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|[{token, 0, 2, Th...|[{word_embeddings...|\n",
      "|See http : //path...|[{document, 0, 30...|[{document, 0, 30...|[{token, 0, 2, Se...|[{pos, 0, 2, VB, ...|[{named_entity, 0...|[{token, 0, 2, Se...|[{word_embeddings...|\n",
      "|pkarp @ ai.sri.com .|[{document, 0, 19...|[{document, 0, 19...|[{token, 0, 4, pk...|[{pos, 0, 4, NN, ...|[{named_entity, 0...|[{token, 0, 4, pk...|[{word_embeddings...|\n",
      "|Supplementary dat...|[{document, 0, 13...|[{document, 0, 13...|[{token, 0, 12, S...|[{pos, 0, 12, NNP...|[{named_entity, 0...|[{token, 0, 12, S...|[{word_embeddings...|\n",
      "|Gene Ontology use...|[{document, 0, 10...|[{document, 0, 10...|[{token, 0, 3, Ge...|[{pos, 0, 3, NNP,...|[{named_entity, 0...|[{token, 0, 3, Ge...|[{word_embeddings...|\n",
      "|However , interpr...|[{document, 0, 13...|[{document, 0, 13...|[{token, 0, 6, Ho...|[{pos, 0, 6, RB, ...|[{named_entity, 0...|[{token, 0, 6, Ho...|[{word_embeddings...|\n",
      "|To address issues...|[{document, 0, 19...|[{document, 0, 19...|[{token, 0, 1, To...|[{pos, 0, 1, TO, ...|[{named_entity, 0...|[{token, 0, 1, To...|[{word_embeddings...|\n",
      "|We tested GOcats ...|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 1, We...|[{pos, 0, 1, PRP,...|[{named_entity, 0...|[{token, 0, 1, We...|[{word_embeddings...|\n",
      "|In comparison ter...|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 1, In...|[{pos, 0, 1, IN, ...|[{named_entity, 0...|[{token, 0, 1, In...|[{word_embeddings...|\n",
      "|Unlike methods , ...|[{document, 0, 63...|[{document, 0, 63...|[{token, 0, 5, Un...|[{pos, 0, 5, IN, ...|[{named_entity, 0...|[{token, 0, 5, Un...|[{word_embeddings...|\n",
      "|biologist ) , man...|[{document, 0, 62...|[{document, 0, 62...|[{token, 0, 8, bi...|[{pos, 0, 8, NN, ...|[{named_entity, 0...|[{token, 0, 8, bi...|[{word_embeddings...|\n",
      "|Additionally , id...|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 11, A...|[{pos, 0, 11, RB,...|[{named_entity, 0...|[{token, 0, 11, A...|[{word_embeddings...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = normalizer.fit(test).transform(test)\n",
    "test = elmo.transform(test)\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write Test Data to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.write.mode('overwrite').parquet(\"Data/testingTextProcessed.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the NER Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_model = ner_pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat /Users/jonathandekermanjian/annotator_logs/NerDLApproach_5b5f74d78e21.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the model onto the test data\n",
    "predictions = ner_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+----------+\n",
      "|word|Truth|Prediction|Confidence|\n",
      "+----+-----+----------+----------+\n",
      "+----+-----+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    predictions\n",
    "    .select(F.explode(F.arrays_zip('token.result','label.result', 'ner.result', 'ner.metadata')).alias('cols'))\n",
    "    .select(F.col('cols.0').alias('word'),\n",
    "            F.col('cols.1').alias('Truth'),\n",
    "            F.col('cols.2').alias('Prediction'),\n",
    "            F.col('cols.3.confidence').alias('Confidence'))\n",
    "    .dropna()\n",
    "    .filter('Truth != \"O\"')\n",
    "    .dropDuplicates(['word', 'Prediction'])\n",
    "    .orderBy(['word','Truth','Prediction'], ascending=False)\n",
    "    .show(100, truncate = False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+----------+\n",
      "|word|Truth|Prediction|Confidence|\n",
      "+----+-----+----------+----------+\n",
      "+----+-----+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    predictions\n",
    "    .select(F.explode(F.arrays_zip('token.result','label.result', 'ner.result', 'ner.metadata')).alias('cols'))\n",
    "    .select(F.col('cols.0').alias('word'),\n",
    "            F.col('cols.1').alias('Truth'),\n",
    "            F.col('cols.2').alias('Prediction'),\n",
    "            F.col('cols.3.confidence').alias('Confidence'))\n",
    "    .dropna()\n",
    "    .filter('Truth != \"T\"')\n",
    "    .dropDuplicates(['word', 'Prediction'])\n",
    "    .orderBy('Prediction', ascending=False)\n",
    "    .show(70, truncate = False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_model.write().overwrite().save(\"NER_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With only 20 epochs, our model is doing pretty descently with an F1 score of 79%. Looking at the logs we seem to be having a harder time with false negatives compared to false positives. However, from the table above you can see that some tools, depending on where within a sentence they are found, are predictied both as a tool (T) and as other (O). Running this model for more epochs and tuning the model hyperparameters should result in a good model for detecting metabolomics software tools with in text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
