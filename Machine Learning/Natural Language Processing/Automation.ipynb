{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sparknlp\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "from sparknlp.common import *\n",
    "import pyspark.sql.functions as F\n",
    "from sparknlp.training import CoNLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to process the data\n",
    "def processText(text):\n",
    "    # Get abstracts into one continuous string\n",
    "    text = text.str.cat()\n",
    "    # Tokenize the string object by word\n",
    "    text = word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    stop = set(stopwords.words('english'))\n",
    "    text = [w for w in text if not w in stop]\n",
    "    # Tag each word by the appropriate part of speech (POS) tag\n",
    "    text = pos_tag(text)\n",
    "    # Reshape the data into a dataframe\n",
    "    text = pd.DataFrame(text, columns=['word','POS'])\n",
    "    return(text)\n",
    "\n",
    "# Define a function that identifies where sentences begin/end\n",
    "def identSentence(textDf):\n",
    "    # start at sentence 1\n",
    "    n_sent = 1\n",
    "    sents = [] # init empty array to wholed sentence identifiers\n",
    "    # Loop through text incrementing n_sent after each period\n",
    "    for word in textDf.word:\n",
    "        if word == \".\":\n",
    "            sents.append(n_sent)\n",
    "            n_sent += 1\n",
    "        else:\n",
    "            sents.append(np.nan) # If we are still before the end of the sentence label it as NA\n",
    "    textDf['Sent_id'] = sents # Generate a column of the sentences \n",
    "    textDf['Sent_id'] = textDf['Sent_id'].bfill() # back fill the NAs to get the correct sentence IDs\n",
    "    return(textDf)\n",
    "\n",
    "# Define a function that reformats the data into CoNLL format\n",
    "def conllFormatter(textDf):\n",
    "    conll_lines = \"-DOCSTART- -X- -X- -O-\\n\\n\"\n",
    "    save = 0\n",
    "\n",
    "    for sent, token, pos, label in zip(textDf['Sent_id'], textDf['word'], textDf['POS'], textDf['label']):\n",
    "        # If we start a new sentence, add empty line.\n",
    "        if save!=sent:\n",
    "            conll_lines+=\"\\n\"\n",
    "        \n",
    "        # Save the line\n",
    "        conll_lines += \"{} {} {} {}\\n\".format(token, pos, pos, label)\n",
    "        save = sent\n",
    "    return(conll_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    noNewAbstracts = pd.read_csv(\"noNewAbstracts.csv\")\n",
    "except:\n",
    "    print(\"New abstracts are available.\")\n",
    "\n",
    "if 'noNewAbstracts' in globals():\n",
    "    sys.exit(0)\n",
    "PubMedAbstracts = pd.read_csv(\"PubMedAbstracts.csv\", usecols=['pmid', 'title_abstract'])\n",
    "text = PubMedAbstracts.title_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = processText(text)\n",
    "text = identSentence(text)\n",
    "text = conllFormatter(text)\n",
    "\n",
    "# Output the processed training data to a txt file\n",
    "with open(\"conll.txt\", \"w\") as txtfile:\n",
    "    for line in text:\n",
    "        txtfile.write(line)\n",
    "txtfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = sparknlp.start(gpu=False)\n",
    "ner_model = PipelineModel.load(\"NER_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the text\n",
    "normalizer = (\n",
    "    Normalizer()\n",
    "    .setInputCols(['token'])\n",
    "    .setOutputCol('normalized')\n",
    "    .setLowercase(False)\n",
    "    .setCleanupPatterns([\"[^\\w\\d\\s]\"])\n",
    ")\n",
    "\n",
    "# Get ELMo word embeddings\n",
    "elmo = (\n",
    "    ElmoEmbeddings.pretrained()\n",
    "    .setInputCols(\"sentence\", \"normalized\")\n",
    "    .setOutputCol(\"elmo\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = CoNLL().readDataset(spark, 'conll.txt')\n",
    "X = normalizer.fit(test).transform(test)\n",
    "X = elmo.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the model onto the test data\n",
    "predictions = ner_model.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (\n",
    "    predictions\n",
    "    .select(F.explode(F.arrays_zip('token.result','label.result', 'ner.result', 'ner.metadata')).alias('cols'))\n",
    "    .select(F.col('cols.0').alias('word'),\n",
    "            F.col('cols.1').alias('Truth'),\n",
    "            F.col('cols.2').alias('Prediction'),\n",
    "            F.col('cols.3.confidence').alias('Confidence'))\n",
    "    .dropna()\n",
    "    .filter('Truth != \"O\"')\n",
    "    .dropDuplicates(['word', 'Prediction'])\n",
    ")\n",
    "\n",
    "predictions = predictions.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if tools are already in database or if they are already deemed as false positives\n",
    "previousTools = None\n",
    "try:\n",
    "    previousTools = pd.read_csv(\"previousTools.csv\")\n",
    "except:\n",
    "    print(\"No file found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "databaseTools = pd.read_csv(\"databaseTools.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potentialTools = predictions[predictions.Prediction == 'T']\n",
    "potentialTools = predictions[~predictions.word.isin(databaseTools)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if previousTools is not None:\n",
    "    previousTools = pd.concat([previousTools.squeeze(), databaseTools.squeeze()], ignore_index=True).drop_duplicates()\n",
    "    newtools = predictions[~predictions.word.isin(previousTools)].dropna()\n",
    "    previousTools = pd.concat([previousTools, newtools], ignore_index = True)\n",
    "    previousTools.to_csv(\"previousTools.csv\", encoding='utf-8', index=False)\n",
    "    tools_output = newtools\n",
    "else:\n",
    "    predictions.to_csv(\"previousTools.csv\", encoding=\"utf-8\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
