{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import classification_report, plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import tensorflow as tf \n",
    "\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./Data/trainingData.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to one hot encode our categorical variables to use in the random forest classifier. We can do so using column transformers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['age', 'avg_glucose_level', 'bmi']\n",
    "categorical_features = ['sex',\n",
    "                        'hypertension',\n",
    "                        'heart_disease',\n",
    "                        'ever_married',\n",
    "                        'work_type',\n",
    "                        'Residence_type',\n",
    "                        'smoking_status',\n",
    "                        'age_group',\n",
    "                        'bmi_group']\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('Numeric', 'passthrough', numeric_features),\n",
    "    ('Categorical', categorical_transformer, categorical_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance the target and make train, validation, and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.pop('stroke').to_numpy()\n",
    "X = preprocessor.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building a classifier it is very important to check whether the classes in your training data are balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.260958205912335"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the class balance\n",
    "100*(sum(y)/len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 4.3% of the subjects in our dataset have a class of stroke, the rest are all no-stroke. This will most likely cause our model to perform very poorly. Let's try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, = train_test_split(X, y, test_size = 0.2) # 20% testing 80% training\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.25) # 25% validation 80% training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-deep-learning method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the model\n",
    "RF = RandomForestClassifier(n_estimators=300)\n",
    "RF = RF.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       940\n",
      "           1       0.00      0.00      0.00        41\n",
      "\n",
      "    accuracy                           0.96       981\n",
      "   macro avg       0.48      0.50      0.49       981\n",
      "weighted avg       0.92      0.96      0.94       981\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = RF.predict(X_test)\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we expected our model only returns predictions that patients in the test data set do not have a stroke, this is due to the heavyily imbalanced class of no-stroke to stroke. Now let's employ a re-sampling technique to address this issue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a heavy class imbalance for people who do not have stroke compared to people who do.\n",
    "# Thus for our classification model to not keep predicting the dominating class we must resample to balance our data\n",
    "# We will use an over-sampling of the minority class using the method Synthetic Minority Oversampling Technique (SMOTE)\n",
    "smote = SMOTE()\n",
    "X_smote, y_smote = smote.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, = train_test_split(X_smote, y_smote, test_size = 0.2) # 20% testing 80% training\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.25) # 25% validation 80% training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the model\n",
    "RF = RandomForestClassifier(n_estimators=300)\n",
    "RF = RF.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       942\n",
      "           1       1.00      0.96      0.98       937\n",
      "\n",
      "    accuracy                           0.98      1879\n",
      "   macro avg       0.98      0.98      0.98      1879\n",
      "weighted avg       0.98      0.98      0.98      1879\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = RF.predict(X_test)\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATgAAAEGCAYAAADxD4m3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAZrUlEQVR4nO3de5QcZZ3/8fdnenIh94QJEJIAAUIwoNzCVUEQlYD8NsiKvwC6HBeXFUHcVWRh/S0ueNiDK6yCKygCGgVBFBRYLgEDLBcx5AaBBENigiGQkAsg5DqZme/vj6rRATI91WQ61V3zeZ1TZ7qrqp/69uTMN8+lnnoUEZiZFVFD3gGYmVWLE5yZFZYTnJkVlhOcmRWWE5yZFVZj3gF01DSsFLuN7pV3GFaBF+b2yzsEq8BG1tEcm7Q1ZRx3TP9Y81prpnNnzd00NSImbs31tkZNJbjdRvfiqamj8w7DKnDczvvnHYJVYHpM2+oy1rzWylNTd8l0bmnEwqatvuBWqKkEZ2a1L4A22vIOIxMnODOrSBBsjmxN1Lw5wZlZxVyDM7NCCoLWOpni6QRnZhVrwwnOzAoogFYnODMrKtfgzKyQAtjsPjgzK6Ig3EQ1s4IKaK2P/OYEZ2aVSWYy1AcnODOrkGhlq+brbzNOcGZWkWSQwQnOzAoouQ/OCc7MCqrNNTgzKyLX4MyssALRWierHTjBmVnF3EQ1s0IKRHOU8g4jEyc4M6tIcqOvm6hmVlAeZDCzQooQreEanJkVVJtrcGZWRMkgQ32kjvqI0sxqhgcZzKzQWn0fnJkVUT3NZKiPKM2sprRFQ6atK5L+WdI8Sc9JukVSX0nDJD0oaWH6c2iH8y+StEjSAknHdVW+E5yZVSSZbN+QaStH0kjgPGBCROwLlIDJwIXAtIgYC0xL3yNpfHp8H2AicI2kslMqnODMrCKB2BylTFsGjcB2khqBfsArwCRgSnp8CnBS+noScGtEbIqIJcAi4JByhTvBmVlFIqA1GjJt5cuJl4ErgKXAcuDPEfEAsGNELE/PWQ7skH5kJPBShyKWpfs65QRnZhUSbRk3oEnSzA7bWX8pJelbmwSMAXYG+kv6TNkLv1vZ9b08impmFQmoZKrW6oiY0MmxjwJLImIVgKQ7gCOAVyWNiIjlkkYAK9PzlwGjO3x+FEmTtlOuwZlZxbpjkIGkaXqYpH6SBBwLPA/cBZyRnnMGcGf6+i5gsqQ+ksYAY4Gnyl3ANTgzq0igbnngZURMl/QrYDbQAswBrgMGALdJOpMkCZ6Snj9P0m3A/PT8cyKitdw1nODMrCLJsoHdkzoi4hvAN96xexNJbW5L518GXJa1fCc4M6uQF342s4IKyDRLoRY4wZlZxVyDM7NCipBrcGZWTMkgg1fVMrNC8poMZlZQySCD++DMrKDq5YGXTnBmVpHumsmwLTjBmVnFvOiMmRVSBGxuc4IzswJKmqhOcGZWUJ7J0IP8+vom7rt5eyLg+NNf4+R/WMWU/9yJJ6cORoIhTZs5/7tL2X6nFjY3i6suGMXCuf1QA5x96cvsd8TavL+CAb36tHHlHYvo1TsoNQaP3TOEn12xU95h1Zx6uk2kqvVMSRPT5b0WSbqwmtfKy4t/6Mt9N2/P1fe8wA9+u4DpDw7i5cW9+dTZK/nBtAVc+9sFHPrRN7npO8kfyn03bw/ADx9awOW3/pHrLtmZtrY8v4G127xJXHDKHpz9sXGc/bFxTDj6LfY+cF3eYdUgdduygdVWtQjS5by+DxwPjAdOTZf9KpSlC/vwvgPX07dfUGqEDxy+lifuG0L/gX/NWhs3NKD0P7ylL/ThgCOTGtuQphYGDG7lhWf65RG6vYvYuD6ZgtTYKyj1CqLsE/97rgrWZMhVNVPsIcCiiFgcEc3ArSQLTBTKbntv5Nnp/XnztRIb14sZDw1i1Su9APjx5Ttx+kHjeeiOofzd15YDsPs+G3ly6mBaW2DF0t4snNvvL+db/hoagmseXMAv5s5jzqMDWDCnf94h1ZxkFLWUactbNRNcpiW+JJ3VvuLOqjVlnz5ck3YZu4lPf3ElF03eg6+fvgdjxm+g1Jj8t/+5C1dw86z5fOTk17nrxuEAHDd5DU0jmjl34jiuvXgk4yeso1RyNaFWtLWJL35sHKcfNJ5x+69n13Eb8g6p5rTf6Jtly1s1E1ymJb4i4rqImBARE4Zvn3/Gfy8mnvYa33/gBa789SIGDmll5JhNbzt+zCdf5/F7BwNQaoQvXPIK1/52AZf8ZAlr/1xi5O6btlSs5WjdmyWeeXIABx/zVt6h1CQ3Ud/DEl/16o3VyWD0ymW9eOLewRx90hu8vLj3X47/fupgRu+ZJLGN68XG9cmvfdb/DqDUGOy6lxNcLRg8rIX+g5JWRO++bRx45FpeWtQ356hqT/soaj3U4Kp5m8gMYGy6vNfLwGTgtCpeLzeXfn433nq9kVKv4Nz/WMbAIa185/zRLPtjHxoaYIeRzZz3rWUAvLGmF18/dXfUANvvtJkLvvennKO3dsN23Mz5Vy2loQEaGuDRuwcz/beD8g6rJtXCCGkWVUtwEdEi6VxgKlACboyIedW6Xp7+6zeL3rXv4utf3OK5O41u5obH/1DliOy9WPL8dpzz8XF5h1HzIkRLT09wABFxL3BvNa9hZtteLTQ/s/BMBjOrSD3NZHCCM7OKOcGZWSH5gZdmVmi1cI9bFk5wZlaRCGjxAy/NrKjcRDWzQnIfnJkVWjjBmVlReZDBzAopwn1wZlZYotWjqGZWVO6DM7NC8lxUMyuuoG4W43GCM7OK1csoan30FJpZzYh0kCHL1hVJQyT9StIfJD0v6XBJwyQ9KGlh+nNoh/MvStdZXiDpuK7Kd4Izs4pFZNsyuAq4PyL2BvYDngcuBKZFxFhgWvqedF3lycA+wETgmnT95U45wZlZxSKUaStH0iDgKOCGpMxojog3SNZPnpKeNgU4KX09Cbg1IjZFxBJgEcn6y51ygjOziiS1s8wJrql93eN0O6tDUbsDq4AfS5oj6XpJ/YEdI2J5cq1YDuyQnp9preWOPMhgZhWr4DaR1RExoZNjjcCBwJciYrqkq0ibo53ItNZyR67BmVnFuqkPbhmwLCKmp+9/RZLwXpU0AiD9ubLD+RWttewEZ2YVCURbW0OmrWw5ESuAlyS1r9V4LDAfuAs4I913BnBn+vouYLKkPul6y2OBp8pdw01UM6tYN97n+yXgZkm9gcXA50gqXrdJOhNYCpwCEBHzJN1GkgRbgHMiorVc4U5wZlaZ6L65qBHxNLClPrpjOzn/MuCyrOU7wZlZ5TxVy8yKqu6fJiLpe5TJ0xFxXlUiMrOaFkBbW50nOGDmNovCzOpHAPVeg4uIKR3fS+ofEeuqH5KZ1bp6eVxSl/fBpbP755NMgkXSfpKuqXpkZla7IuOWsyw3+n4XOA5YAxARz5BMkDWzHinbPNRaGIjINIoaES9Jbwu27M11ZlZwNVA7yyJLgntJ0hFApHcbn0faXDWzHigg6mQUNUsT9QvAOSSPJXkZ2D99b2Y9ljJu+eqyBhcRq4HTt0EsZlYv6qSJmmUUdXdJd0taJWmlpDsl7b4tgjOzGlWgUdSfA7cBI4CdgV8Ct1QzKDOrYe03+mbZcpYlwSkifhYRLel2EzWRm80sL9246ExVlZuLOix9+bCkC4FbSRLb/wXu2QaxmVmtqpNR1HKDDLNIElr7N/nHDscC+Ga1gjKz2qYaqJ1lUW4u6phtGYiZ1YkaGUDIItNMBkn7AuOBvu37IuKn1QrKzGpZbQwgZNFlgpP0DeBokgR3L3A88DjgBGfWU9VJDS7LKOqnSJ6PviIiPgfsB/SpalRmVtvaMm45y9JE3RARbZJaJA0iWaPQN/qa9VRFeOBlBzMlDQF+RDKyupYu1iI0s2Kr+1HUdhHxxfTlDyTdDwyKiLnVDcvMalq9JzhJB5Y7FhGzqxOSmVn3KFeDu7LMsQA+0s2xsPC5ARw/7sjuLtaq6Ial9+UdglXgxBPe6pZy6r6JGhHHbMtAzKxOBIWYqmVmtmX1XoMzM+tM3TdRzcw6VScJLssTfSXpM5IuTt/vIumQ6odmZjWrQE/0vQY4HDg1ff8W8P2qRWRmNU2RfctblibqoRFxoKQ5ABHxerp8oJn1VAUaRd0sqURa4ZQ0nJqYRmtmeamF2lkWWZqoVwO/BnaQdBnJo5L+o6pRmVltq5M+uCxzUW+WNIvkkUkCTooIr2xv1lPVSP9aFlkeeLkLsB64u+O+iFhazcDMrIYVJcGRrKDVvvhMX2AMsADYp4pxmVkNU530wnfZBxcR74+ID6Q/xwKHkPTDmZltNUklSXMk/U/6fpikByUtTH8O7XDuRZIWSVog6biuys4yyPA26WOSDq70c2ZWIN07yPBloGO//oXAtLRCNS19j6TxwGSS1uNE4Jr0Do9OZemD+0qHtw3AgcCqzKGbWbF04yCDpFHAJ4DLgPZcM4lkoSuAKcAjwL+k+2+NiE3AEkmLSFqUT3ZWfpYa3MAOWx+SPrlJFX4PMyuS7qvBfRe4gLffW7tjRCwHSH/ukO4fCbzU4bxl6b5Ola3BpdW/ARHxtUyhmlnPkL0G1yRpZof310XEdQCSTgRWRsQsSUdnKGtL0yfKRlLukeWNEdFS7tHlZtbziIpGUVdHxIROjn0Q+BtJJ5DcoTFI0k3Aq5JGRMRySSNIVvKDpMY2usPnRwGvlLt4uSZq+8pZT0u6S9JnJZ3cvnX1rcysoLppsn1EXBQRoyJiN5LBg4ci4jPAXcAZ6WlnAHemr+8CJkvqI2kMMJYuVvjLch/cMGANyRoM7ffDBXBHhs+aWRFV90bfy4HbJJ0JLAVOAYiIeZJuA+YDLcA5EdFarqByCW6HdAT1Of6a2NrVyX3MZlYV3ZwBIuIRktFSImINydTQLZ13GcmIayblElwJGMB76Ngzs2IrwlzU5RFx6TaLxMzqRwESXH080c7Mtq2on7mo5RLcFtvAZmZ1X4OLiNe2ZSBmVj+K0AdnZrZlTnBmVkg18jjyLJzgzKwiwk1UMyswJzgzKy4nODMrLCc4MyukIi0baGb2Lk5wZlZURZiqZWa2RW6imlkx+UZfMys0JzgzKyLPZDCzQlNbfWQ4Jzgzq4z74MysyNxENbPicoIzs6JyDc7MissJzswKqSCrapmZvYvvgzOzYov6yHBOcGZWMdfgeqBevdv49s1z6dW7jVIJHp+6PTd9b1fGjFvLly75I337tbLy5T785/njWL/Ov/o8PXD9zjx2y44gGLX3ev7+ihdYsbgfP/3XPdi0rkTTqE38w9UL2G5gKy2bxZQL9uRPzw2grVUcfvJKPnHusry/Qn58oy9IuhE4EVgZEftW6zq1ZHOzuPCM97NxfYlSYxtX/HwuMx8dytn/tpjrvzWGZ2cM5uN/u4K//fzL/OyqXfMOt8d6fUVvpv14Z745bTa9+7Zx7dnjmH73cB6eMoJP/78ljDvsTR77xY7c/8ORfPL8pcy8p4nNzQ1c+uAcNm1o4N+OPZBDJ62iafSmvL9KbuplkKGhimX/BJhYxfJrkNi4vgRAY2PQ2BhEiFFjNvDsjEEAzH5iKB/6+Oo8gzSgtUU0b2ygtQWaN5QYsmMzKxZvx16HvgnAPke+zqx7mwCQgub1JVpbYPPGBhp7BX0HtuYZfu7Ulm3LW9USXEQ8CrxWrfJrVUND8N+/mcMtv5vOnN8NYcHcgbz4Qj8OOzb5VRw5cTVNI5pzjrJnG7pTM8ed9TIXHHYwX5lwKNsNamHfo95g5Lj1PP3gMABm3NPEa8t7A3DQCWvo3a+Vr0w4lK8ddjDHnbWMAUNa8vwK+QqSQYYsW86qWYPLRNJZkmZKmtkcG/MOZ6u1tYlzTzqAz374EPb6wFp2HbuO73x9LP/ntOVcffsctuvfSkuz8g6zR1v3RomnHxzGt56YwZUznmLT+hJP3jGcz317IQ9NGcGlJ+zPxrUlGnslf6BLnh5AQym4csZTfOuJmUz90UhW/alPzt8iX4psW95y7+mOiOuA6wAGl5pq4FfSPda91cjc6YOZcOTr3H7jKL5+ZtINOXK3DRxydI+r2NaU+Y8PoWn0RgZun9TCDpq4hkWzBnH4yav46s3zAFixuC/PPpTU5qbfOZx9P/w6jb2CQU2b2XPCW7w4dyDDd+25fXD1MsiQew2uSAYP3Uz/gckfTe8+rRxwxBu8tLgfg4clTVIpmHz2Uu69dac8w+zxth+5icWzB7JpQwMR8PwTg9l5z/W8uboXAG1t8D9X78KHP7MCgGE7b+IPvxtCBGxa38Di2QPZac/1eX6FXLXf6OsaXA8zdIdmzr/8BRpKgQSP3d/EU48MY9LfvcyJpy0H4HcPNvHA7TvmHGnPtvsBaznohDVcesL+NJSCXfZZx1GnreCRm0bw8E9HAHDgxNV86NOvAvCRM5Zz41f34uKPHkCE+NCnX2X0+3pugiOibh54qahSR6CkW4CjgSbgVeAbEXFDuc8MLjXFYQP+pirxWHX8aN59eYdgFTjxhNXMnbt5qzqBBw4ZFQcc9eVM5z529wWzImLC1lxva1StBhcRp1arbDPLVy00P7NwH5yZVSaAtsi2lSFptKSHJT0vaZ6kL6f7h0l6UNLC9OfQDp+5SNIiSQskHddVqE5wZla5yLiV1wJ8NSLeBxwGnCNpPHAhMC0ixgLT0vekxyYD+5BMIrhGUqncBZzgzKxi3TGKGhHLI2J2+vot4HlgJDAJmJKeNgU4KX09Cbg1IjZFxBJgEXBIuWt4FNXMKlbBKGqTpJkd3l+X3vv69vKk3YADgOnAjhGxHJIkKGmH9LSRwO87fGxZuq9TTnBmVpnKniayuqtRVEkDgNuBf4qIN6VOB3m3dKBsJG6imllFkht9I9PWZVlSL5LkdnNE3JHuflXSiPT4CGBlun8ZMLrDx0cBr5Qr3wnOzCrXlnErQ0lV7Qbg+Yj4rw6H7gLOSF+fAdzZYf9kSX0kjQHGAk+Vu4abqGZWsSy1sww+CHwWeFbS0+m+fwUuB26TdCawFDgFICLmSboNmE8yAntORJR9bpUTnJlVppue6BsRj7PlfjWAYzv5zGXAZVmv4QRnZhWqn7moTnBmVrkaeJhlFk5wZlYZL/xsZoXmGpyZFVZ95DcnODOrnNrqo43qBGdmlQm6vIm3VjjBmVlFRLZpWLXACc7MKucEZ2aF5QRnZoXkPjgzKzKPoppZQYWbqGZWUIETnJkVWH20UJ3gzKxyvg/OzIrLCc7MCikCWuujjeoEZ2aVcw3OzArLCc7MCikAr8lgZsUUEO6DM7MiCjzIYGYF5j44MyssJzgzKyZPtjezogrAj0sys8JyDc7MislTtcysqALC98GZWWF5JoOZFZb74MyskCI8impmBeYanJkVUxCtrXkHkYkTnJlVxo9LMrNC820iZlZEAYRrcGZWSOEHXppZgdXLIIOihoZ7Ja0C/pR3HFXQBKzOOwirSFH/zXaNiOFbU4Ck+0l+P1msjoiJW3O9rVFTCa6oJM2MiAl5x2HZ+d+sGBryDsDMrFqc4MyssJzgto3r8g7AKuZ/swJwH5yZFZZrcGZWWE5wZlZYTnBVJGmipAWSFkm6MO94rGuSbpS0UtJzecdiW88JrkoklYDvA8cD44FTJY3PNyrL4CdAbjemWvdygqueQ4BFEbE4IpqBW4FJOcdkXYiIR4HX8o7DuocTXPWMBF7q8H5Zus/MthEnuOrRFvb5nhyzbcgJrnqWAaM7vB8FvJJTLGY9khNc9cwAxkoaI6k3MBm4K+eYzHoUJ7gqiYgW4FxgKvA8cFtEzMs3KuuKpFuAJ4FxkpZJOjPvmOy981QtMyss1+DMrLCc4MyssJzgzKywnODMrLCc4MyssJzg6oikVklPS3pO0i8l9duKsn4i6VPp6+vLPQhA0tGSjngP13hR0rtWX+ps/zvOWVvhtf5d0vmVxmjF5gRXXzZExP4RsS/QDHyh48H0CSYVi4jPR8T8MqccDVSc4Mzy5gRXvx4D9kxrVw9L+jnwrKSSpG9LmiFprqR/BFDivyXNl3QPsEN7QZIekTQhfT1R0mxJz0iaJmk3kkT6z2nt8UhJwyXdnl5jhqQPpp/dXtIDkuZI+iFbno/7NpJ+I2mWpHmSznrHsSvTWKZJGp7u20PS/elnHpO0d3f8Mq2YvLJ9HZLUSPKcufvTXYcA+0bEkjRJ/DkiDpbUB3hC0gPAAcA44P3AjsB84MZ3lDsc+BFwVFrWsIh4TdIPgLURcUV63s+B70TE45J2IZmt8T7gG8DjEXGppE8Ab0tYnfj79BrbATMk3R4Ra4D+wOyI+Kqki9OyzyVZDOYLEbFQ0qHANcBH3sOv0XoAJ7j6sp2kp9PXjwE3kDQdn4qIJen+jwMfaO9fAwYDY4GjgFsiohV4RdJDWyj/MODR9rIiorPnon0UGC/9pYI2SNLA9Bonp5+9R9LrGb7TeZI+mb4enca6BmgDfpHuvwm4Q9KA9Pv+ssO1+2S4hvVQTnD1ZUNE7N9xR/qHvq7jLuBLETH1HeedQNePa1KGcyDp2jg8IjZsIZbMc/8kHU2SLA+PiPWSHgH6dnJ6pNd9452/A7POuA+ueKYCZ0vqBSBpL0n9gUeByWkf3QjgmC189kngw5LGpJ8dlu5/CxjY4bwHSJqLpOe1J5xHgdPTfccDQ7uIdTDweprc9iapQbZrANproaeRNH3fBJZIOiW9hiTt18U1rAdzgiue60n612anC6f8kKSm/mtgIfAscC3wv+/8YESsIuk3u0PSM/y1iXg38Mn2QQbgPGBCOogxn7+O5l4CHCVpNklTeWkXsd4PNEqaC3wT+H2HY+uAfSTNIuljuzTdfzpwZhrfPPwYeCvDTxMxs8JyDc7MCssJzswKywnOzArLCc7MCssJzswKywnOzArLCc7MCuv/A/iiD4cbHmPRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax=plot_confusion_matrix(RF, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, this is significantly better! After addressing the class imbalance the performance of our model is very good. The confusion matrix shows the true positives (898), true negatives (939), false negatives (39) and the false positives (3). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep-learning Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can do better using a deep-learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the data into correct batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "test = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "val = tf.data.Dataset.from_tensor_slices((X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.shuffle(buffer_size=1024).batch(32).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "test = test.batch(32).prefetch(1)\n",
    "val = val.batch(32).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((None, 31), (None,)), types: (tf.float64, tf.int64)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the input shape\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 512)               16384     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,067,009\n",
      "Trainable params: 1,067,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define a very simple neural network with a dropout layer for regularization (this simulates L2 regularization)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape = (31)),\n",
    "    tf.keras.layers.Dense(units=512, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(units = 1024, activation = \"relu\"),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(units=512, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(units = 1, activation=\"sigmoid\")\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.8557 - accuracy: 0.7132 - val_loss: 0.5391 - val_accuracy: 0.7014\n",
      "Epoch 2/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.4890 - accuracy: 0.7457 - val_loss: 0.4770 - val_accuracy: 0.7578\n",
      "Epoch 3/200\n",
      "177/177 [==============================] - 3s 18ms/step - loss: 0.4791 - accuracy: 0.7524 - val_loss: 0.4649 - val_accuracy: 0.7674\n",
      "Epoch 4/200\n",
      "177/177 [==============================] - 3s 18ms/step - loss: 0.4600 - accuracy: 0.7645 - val_loss: 0.4676 - val_accuracy: 0.7791\n",
      "Epoch 5/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.4621 - accuracy: 0.7671 - val_loss: 0.4480 - val_accuracy: 0.7866\n",
      "Epoch 6/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.4476 - accuracy: 0.7735 - val_loss: 0.4451 - val_accuracy: 0.7871\n",
      "Epoch 7/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.4400 - accuracy: 0.7794 - val_loss: 0.4361 - val_accuracy: 0.7951\n",
      "Epoch 8/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.4388 - accuracy: 0.7845 - val_loss: 0.4470 - val_accuracy: 0.7813\n",
      "Epoch 9/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.4324 - accuracy: 0.7879 - val_loss: 0.5547 - val_accuracy: 0.7392\n",
      "Epoch 10/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.4327 - accuracy: 0.7872 - val_loss: 0.4344 - val_accuracy: 0.7866\n",
      "Epoch 11/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.4269 - accuracy: 0.7941 - val_loss: 0.4339 - val_accuracy: 0.7935\n",
      "Epoch 12/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.4162 - accuracy: 0.7993 - val_loss: 0.4292 - val_accuracy: 0.7967\n",
      "Epoch 13/200\n",
      "177/177 [==============================] - 3s 18ms/step - loss: 0.4115 - accuracy: 0.8056 - val_loss: 0.4064 - val_accuracy: 0.8206\n",
      "Epoch 14/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.4046 - accuracy: 0.8039 - val_loss: 0.4181 - val_accuracy: 0.8137\n",
      "Epoch 15/200\n",
      "177/177 [==============================] - 3s 18ms/step - loss: 0.4049 - accuracy: 0.8040 - val_loss: 0.4160 - val_accuracy: 0.8047\n",
      "Epoch 16/200\n",
      "177/177 [==============================] - 3s 19ms/step - loss: 0.4113 - accuracy: 0.8009 - val_loss: 0.4029 - val_accuracy: 0.8201\n",
      "Epoch 17/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.3973 - accuracy: 0.8117 - val_loss: 0.4065 - val_accuracy: 0.8116\n",
      "Epoch 18/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.3984 - accuracy: 0.8154 - val_loss: 0.4095 - val_accuracy: 0.8185\n",
      "Epoch 19/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.3906 - accuracy: 0.8127 - val_loss: 0.3864 - val_accuracy: 0.8276\n",
      "Epoch 20/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.3825 - accuracy: 0.8197 - val_loss: 0.3876 - val_accuracy: 0.8286\n",
      "Epoch 21/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.3773 - accuracy: 0.8253 - val_loss: 0.4912 - val_accuracy: 0.8057\n",
      "Epoch 22/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.3831 - accuracy: 0.8216 - val_loss: 0.3858 - val_accuracy: 0.8222\n",
      "Epoch 23/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.3726 - accuracy: 0.8285 - val_loss: 0.4035 - val_accuracy: 0.8148\n",
      "Epoch 24/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.3625 - accuracy: 0.8367 - val_loss: 0.3547 - val_accuracy: 0.8483\n",
      "Epoch 25/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.3639 - accuracy: 0.8360 - val_loss: 0.4451 - val_accuracy: 0.7972\n",
      "Epoch 26/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.3632 - accuracy: 0.8349 - val_loss: 0.3612 - val_accuracy: 0.8414\n",
      "Epoch 27/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.3659 - accuracy: 0.8340 - val_loss: 0.3558 - val_accuracy: 0.8414\n",
      "Epoch 28/200\n",
      "177/177 [==============================] - 3s 18ms/step - loss: 0.3539 - accuracy: 0.8422 - val_loss: 0.3435 - val_accuracy: 0.8462\n",
      "Epoch 29/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.3488 - accuracy: 0.8429 - val_loss: 0.3596 - val_accuracy: 0.8403\n",
      "Epoch 30/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.3414 - accuracy: 0.8484 - val_loss: 0.3776 - val_accuracy: 0.8313\n",
      "Epoch 31/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.3306 - accuracy: 0.8532 - val_loss: 0.3297 - val_accuracy: 0.8606\n",
      "Epoch 32/200\n",
      "177/177 [==============================] - 3s 18ms/step - loss: 0.3231 - accuracy: 0.8578 - val_loss: 0.3299 - val_accuracy: 0.8627\n",
      "Epoch 33/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.3152 - accuracy: 0.8651 - val_loss: 0.3106 - val_accuracy: 0.8675\n",
      "Epoch 34/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.3242 - accuracy: 0.8591 - val_loss: 0.3210 - val_accuracy: 0.8627\n",
      "Epoch 35/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.3018 - accuracy: 0.8722 - val_loss: 0.4101 - val_accuracy: 0.8222\n",
      "Epoch 36/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.3091 - accuracy: 0.8708 - val_loss: 0.3258 - val_accuracy: 0.8611\n",
      "Epoch 37/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.2915 - accuracy: 0.8786 - val_loss: 0.3041 - val_accuracy: 0.8717\n",
      "Epoch 38/200\n",
      "177/177 [==============================] - 3s 18ms/step - loss: 0.2817 - accuracy: 0.8836 - val_loss: 0.3016 - val_accuracy: 0.8733\n",
      "Epoch 39/200\n",
      "177/177 [==============================] - 3s 18ms/step - loss: 0.2856 - accuracy: 0.8782 - val_loss: 0.3253 - val_accuracy: 0.8760\n",
      "Epoch 40/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.2894 - accuracy: 0.8782 - val_loss: 0.3019 - val_accuracy: 0.8728\n",
      "Epoch 41/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.2627 - accuracy: 0.8921 - val_loss: 0.2967 - val_accuracy: 0.8845\n",
      "Epoch 42/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.2635 - accuracy: 0.8916 - val_loss: 0.2971 - val_accuracy: 0.8909\n",
      "Epoch 43/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.2554 - accuracy: 0.8953 - val_loss: 0.2718 - val_accuracy: 0.8936\n",
      "Epoch 44/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.2660 - accuracy: 0.8905 - val_loss: 0.2779 - val_accuracy: 0.8872\n",
      "Epoch 45/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.2537 - accuracy: 0.8960 - val_loss: 0.2701 - val_accuracy: 0.8930\n",
      "Epoch 46/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.2521 - accuracy: 0.8974 - val_loss: 0.3608 - val_accuracy: 0.8611\n",
      "Epoch 47/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.2561 - accuracy: 0.8930 - val_loss: 0.2575 - val_accuracy: 0.8999\n",
      "Epoch 48/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.2327 - accuracy: 0.9040 - val_loss: 0.2552 - val_accuracy: 0.9037\n",
      "Epoch 49/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.2350 - accuracy: 0.9047 - val_loss: 0.2997 - val_accuracy: 0.8739\n",
      "Epoch 50/200\n",
      "177/177 [==============================] - 3s 18ms/step - loss: 0.2346 - accuracy: 0.9052 - val_loss: 0.2777 - val_accuracy: 0.8877\n",
      "Epoch 51/200\n",
      "177/177 [==============================] - 4s 20ms/step - loss: 0.2332 - accuracy: 0.9063 - val_loss: 0.2562 - val_accuracy: 0.8999\n",
      "Epoch 52/200\n",
      "177/177 [==============================] - 3s 18ms/step - loss: 0.2264 - accuracy: 0.9081 - val_loss: 0.2380 - val_accuracy: 0.9101\n",
      "Epoch 53/200\n",
      "177/177 [==============================] - 3s 18ms/step - loss: 0.2067 - accuracy: 0.9201 - val_loss: 0.2481 - val_accuracy: 0.9021\n",
      "Epoch 54/200\n",
      "177/177 [==============================] - 3s 18ms/step - loss: 0.2055 - accuracy: 0.9184 - val_loss: 0.2491 - val_accuracy: 0.9015\n",
      "Epoch 55/200\n",
      "177/177 [==============================] - 3s 19ms/step - loss: 0.2043 - accuracy: 0.9191 - val_loss: 0.3022 - val_accuracy: 0.8861\n",
      "Epoch 56/200\n",
      "177/177 [==============================] - 3s 18ms/step - loss: 0.2002 - accuracy: 0.9230 - val_loss: 0.2652 - val_accuracy: 0.8989\n",
      "Epoch 57/200\n",
      "177/177 [==============================] - 3s 19ms/step - loss: 0.3216 - accuracy: 0.8559 - val_loss: 0.3182 - val_accuracy: 0.8648\n",
      "Epoch 58/200\n",
      "177/177 [==============================] - 3s 19ms/step - loss: 0.2705 - accuracy: 0.8852 - val_loss: 0.2942 - val_accuracy: 0.8728\n",
      "Epoch 59/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.2311 - accuracy: 0.9043 - val_loss: 0.2423 - val_accuracy: 0.9106\n",
      "Epoch 60/200\n",
      "177/177 [==============================] - 3s 18ms/step - loss: 0.2167 - accuracy: 0.9175 - val_loss: 0.2358 - val_accuracy: 0.9101\n",
      "Epoch 61/200\n",
      "177/177 [==============================] - 3s 18ms/step - loss: 0.2047 - accuracy: 0.9178 - val_loss: 0.2270 - val_accuracy: 0.9111\n",
      "Epoch 62/200\n",
      "177/177 [==============================] - 3s 18ms/step - loss: 0.1863 - accuracy: 0.9288 - val_loss: 0.2545 - val_accuracy: 0.9063\n",
      "Epoch 63/200\n",
      "177/177 [==============================] - 3s 18ms/step - loss: 0.1909 - accuracy: 0.9285 - val_loss: 0.2076 - val_accuracy: 0.9234\n",
      "Epoch 64/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.1815 - accuracy: 0.9304 - val_loss: 0.2102 - val_accuracy: 0.9218\n",
      "Epoch 65/200\n",
      "177/177 [==============================] - 3s 18ms/step - loss: 0.1661 - accuracy: 0.9366 - val_loss: 0.2211 - val_accuracy: 0.9207\n",
      "Epoch 66/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.1647 - accuracy: 0.9373 - val_loss: 0.2371 - val_accuracy: 0.9117\n",
      "Epoch 67/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.1852 - accuracy: 0.9292 - val_loss: 0.2183 - val_accuracy: 0.9196\n",
      "Epoch 68/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.1618 - accuracy: 0.9375 - val_loss: 0.2304 - val_accuracy: 0.9228\n",
      "Epoch 69/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.1642 - accuracy: 0.9365 - val_loss: 0.2289 - val_accuracy: 0.9212\n",
      "Epoch 70/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.1542 - accuracy: 0.9432 - val_loss: 0.1839 - val_accuracy: 0.9351\n",
      "Epoch 71/200\n",
      "177/177 [==============================] - 3s 19ms/step - loss: 0.1536 - accuracy: 0.9437 - val_loss: 0.1958 - val_accuracy: 0.9260\n",
      "Epoch 72/200\n",
      "177/177 [==============================] - 4s 21ms/step - loss: 0.1543 - accuracy: 0.9411 - val_loss: 0.2068 - val_accuracy: 0.9212\n",
      "Epoch 73/200\n",
      "177/177 [==============================] - 3s 18ms/step - loss: 0.1406 - accuracy: 0.9489 - val_loss: 0.1814 - val_accuracy: 0.9340\n",
      "Epoch 74/200\n",
      "177/177 [==============================] - 3s 18ms/step - loss: 0.1456 - accuracy: 0.9471 - val_loss: 0.2323 - val_accuracy: 0.9143\n",
      "Epoch 75/200\n",
      "177/177 [==============================] - 3s 19ms/step - loss: 0.1475 - accuracy: 0.9468 - val_loss: 0.1975 - val_accuracy: 0.9276\n",
      "Epoch 76/200\n",
      "177/177 [==============================] - 3s 19ms/step - loss: 0.1289 - accuracy: 0.9508 - val_loss: 0.1781 - val_accuracy: 0.9356\n",
      "Epoch 77/200\n",
      "177/177 [==============================] - 3s 18ms/step - loss: 0.1370 - accuracy: 0.9503 - val_loss: 0.1706 - val_accuracy: 0.9383\n",
      "Epoch 78/200\n",
      "177/177 [==============================] - 3s 18ms/step - loss: 0.1340 - accuracy: 0.9485 - val_loss: 0.1827 - val_accuracy: 0.9335\n",
      "Epoch 79/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.1368 - accuracy: 0.9498 - val_loss: 0.1705 - val_accuracy: 0.9377\n",
      "Epoch 80/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.1285 - accuracy: 0.9524 - val_loss: 0.1899 - val_accuracy: 0.9356\n",
      "Epoch 81/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.1415 - accuracy: 0.9464 - val_loss: 0.1886 - val_accuracy: 0.9335\n",
      "Epoch 82/200\n",
      "177/177 [==============================] - 3s 18ms/step - loss: 0.1235 - accuracy: 0.9542 - val_loss: 0.1833 - val_accuracy: 0.9415\n",
      "Epoch 83/200\n",
      "177/177 [==============================] - 3s 18ms/step - loss: 0.1337 - accuracy: 0.9499 - val_loss: 0.1846 - val_accuracy: 0.9367\n",
      "Epoch 84/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.1309 - accuracy: 0.9512 - val_loss: 0.1849 - val_accuracy: 0.9308\n",
      "Epoch 85/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.1209 - accuracy: 0.9533 - val_loss: 0.1885 - val_accuracy: 0.9388\n",
      "Epoch 86/200\n",
      "177/177 [==============================] - 3s 19ms/step - loss: 0.1337 - accuracy: 0.9496 - val_loss: 0.1706 - val_accuracy: 0.9478\n",
      "Epoch 87/200\n",
      "177/177 [==============================] - 3s 19ms/step - loss: 0.1064 - accuracy: 0.9610 - val_loss: 0.2069 - val_accuracy: 0.9260\n",
      "Epoch 88/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.1082 - accuracy: 0.9611 - val_loss: 0.2180 - val_accuracy: 0.9271\n",
      "Epoch 89/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.1337 - accuracy: 0.9515 - val_loss: 0.1884 - val_accuracy: 0.9335\n",
      "Epoch 90/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.1092 - accuracy: 0.9574 - val_loss: 0.1670 - val_accuracy: 0.9447\n",
      "Epoch 91/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.1055 - accuracy: 0.9594 - val_loss: 0.1783 - val_accuracy: 0.9468\n",
      "Epoch 92/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.1036 - accuracy: 0.9640 - val_loss: 0.1469 - val_accuracy: 0.9537\n",
      "Epoch 93/200\n",
      "177/177 [==============================] - 3s 14ms/step - loss: 0.1064 - accuracy: 0.9640 - val_loss: 0.1739 - val_accuracy: 0.9452\n",
      "Epoch 94/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.1216 - accuracy: 0.9549 - val_loss: 0.2205 - val_accuracy: 0.9266\n",
      "Epoch 95/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.1107 - accuracy: 0.9599 - val_loss: 0.1725 - val_accuracy: 0.9335\n",
      "Epoch 96/200\n",
      "177/177 [==============================] - 3s 18ms/step - loss: 0.1123 - accuracy: 0.9581 - val_loss: 0.1559 - val_accuracy: 0.9399\n",
      "Epoch 97/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.1060 - accuracy: 0.9588 - val_loss: 0.1870 - val_accuracy: 0.9377\n",
      "Epoch 98/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.1005 - accuracy: 0.9611 - val_loss: 0.2166 - val_accuracy: 0.9292\n",
      "Epoch 99/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.1220 - accuracy: 0.9562 - val_loss: 0.1821 - val_accuracy: 0.9367\n",
      "Epoch 100/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.1107 - accuracy: 0.9586 - val_loss: 0.1387 - val_accuracy: 0.9558\n",
      "Epoch 101/200\n",
      "177/177 [==============================] - 3s 18ms/step - loss: 0.1037 - accuracy: 0.9636 - val_loss: 0.1680 - val_accuracy: 0.9409\n",
      "Epoch 102/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.0985 - accuracy: 0.9622 - val_loss: 0.1626 - val_accuracy: 0.9415\n",
      "Epoch 103/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.0865 - accuracy: 0.9693 - val_loss: 0.1801 - val_accuracy: 0.9473\n",
      "Epoch 104/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.1131 - accuracy: 0.9608 - val_loss: 0.1745 - val_accuracy: 0.9393\n",
      "Epoch 105/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0844 - accuracy: 0.9696 - val_loss: 0.1504 - val_accuracy: 0.9489\n",
      "Epoch 106/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.0761 - accuracy: 0.9727 - val_loss: 0.1433 - val_accuracy: 0.9532\n",
      "Epoch 107/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.1168 - accuracy: 0.9563 - val_loss: 0.1634 - val_accuracy: 0.9473\n",
      "Epoch 108/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0853 - accuracy: 0.9693 - val_loss: 0.1570 - val_accuracy: 0.9468\n",
      "Epoch 109/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0943 - accuracy: 0.9657 - val_loss: 0.1626 - val_accuracy: 0.9425\n",
      "Epoch 110/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.0831 - accuracy: 0.9704 - val_loss: 0.1814 - val_accuracy: 0.9441\n",
      "Epoch 111/200\n",
      "177/177 [==============================] - 3s 18ms/step - loss: 0.0788 - accuracy: 0.9718 - val_loss: 0.1584 - val_accuracy: 0.9510\n",
      "Epoch 112/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0931 - accuracy: 0.9659 - val_loss: 0.1667 - val_accuracy: 0.9468\n",
      "Epoch 113/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.0926 - accuracy: 0.9645 - val_loss: 0.1660 - val_accuracy: 0.9393\n",
      "Epoch 114/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.0751 - accuracy: 0.9732 - val_loss: 0.1530 - val_accuracy: 0.9564\n",
      "Epoch 115/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0798 - accuracy: 0.9730 - val_loss: 0.2113 - val_accuracy: 0.9372\n",
      "Epoch 116/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0930 - accuracy: 0.9679 - val_loss: 0.1773 - val_accuracy: 0.9409\n",
      "Epoch 117/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.2248 - accuracy: 0.9155 - val_loss: 0.2717 - val_accuracy: 0.8898\n",
      "Epoch 118/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.1364 - accuracy: 0.9489 - val_loss: 0.1993 - val_accuracy: 0.9250\n",
      "Epoch 119/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.1058 - accuracy: 0.9617 - val_loss: 0.1876 - val_accuracy: 0.9255\n",
      "Epoch 120/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0974 - accuracy: 0.9638 - val_loss: 0.1805 - val_accuracy: 0.9404\n",
      "Epoch 121/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0879 - accuracy: 0.9679 - val_loss: 0.1927 - val_accuracy: 0.9383\n",
      "Epoch 122/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0958 - accuracy: 0.9627 - val_loss: 0.2176 - val_accuracy: 0.9345\n",
      "Epoch 123/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0918 - accuracy: 0.9684 - val_loss: 0.2041 - val_accuracy: 0.9372\n",
      "Epoch 124/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.1167 - accuracy: 0.9578 - val_loss: 0.2153 - val_accuracy: 0.9255\n",
      "Epoch 125/200\n",
      "177/177 [==============================] - 3s 18ms/step - loss: 0.1123 - accuracy: 0.9570 - val_loss: 0.1442 - val_accuracy: 0.9494\n",
      "Epoch 126/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0706 - accuracy: 0.9759 - val_loss: 0.1518 - val_accuracy: 0.9553\n",
      "Epoch 127/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0869 - accuracy: 0.9691 - val_loss: 0.1976 - val_accuracy: 0.9223\n",
      "Epoch 128/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0898 - accuracy: 0.9670 - val_loss: 0.2151 - val_accuracy: 0.9388\n",
      "Epoch 129/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0699 - accuracy: 0.9755 - val_loss: 0.2102 - val_accuracy: 0.9441\n",
      "Epoch 130/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0748 - accuracy: 0.9736 - val_loss: 0.1733 - val_accuracy: 0.9436\n",
      "Epoch 131/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0716 - accuracy: 0.9743 - val_loss: 0.2106 - val_accuracy: 0.9399\n",
      "Epoch 132/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0760 - accuracy: 0.9720 - val_loss: 0.2375 - val_accuracy: 0.9324\n",
      "Epoch 133/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.0925 - accuracy: 0.9668 - val_loss: 0.1662 - val_accuracy: 0.9505\n",
      "Epoch 134/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.1014 - accuracy: 0.9633 - val_loss: 0.1857 - val_accuracy: 0.9431\n",
      "Epoch 135/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0851 - accuracy: 0.9686 - val_loss: 0.1605 - val_accuracy: 0.9505\n",
      "Epoch 136/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0649 - accuracy: 0.9762 - val_loss: 0.1577 - val_accuracy: 0.9494\n",
      "Epoch 137/200\n",
      "177/177 [==============================] - 3s 18ms/step - loss: 0.0685 - accuracy: 0.9727 - val_loss: 0.1772 - val_accuracy: 0.9484\n",
      "Epoch 138/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.0901 - accuracy: 0.9698 - val_loss: 0.1745 - val_accuracy: 0.9415\n",
      "Epoch 139/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0609 - accuracy: 0.9787 - val_loss: 0.1769 - val_accuracy: 0.9473\n",
      "Epoch 140/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0763 - accuracy: 0.9725 - val_loss: 0.2033 - val_accuracy: 0.9420\n",
      "Epoch 141/200\n",
      "177/177 [==============================] - 3s 18ms/step - loss: 0.1003 - accuracy: 0.9668 - val_loss: 0.1843 - val_accuracy: 0.9478\n",
      "Epoch 142/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0596 - accuracy: 0.9776 - val_loss: 0.1545 - val_accuracy: 0.9505\n",
      "Epoch 143/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.0646 - accuracy: 0.9750 - val_loss: 0.1648 - val_accuracy: 0.9473\n",
      "Epoch 144/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.0775 - accuracy: 0.9737 - val_loss: 0.2348 - val_accuracy: 0.9287\n",
      "Epoch 145/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0819 - accuracy: 0.9700 - val_loss: 0.1788 - val_accuracy: 0.9436\n",
      "Epoch 146/200\n",
      "177/177 [==============================] - 3s 18ms/step - loss: 0.0639 - accuracy: 0.9762 - val_loss: 0.1383 - val_accuracy: 0.9580\n",
      "Epoch 147/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0665 - accuracy: 0.9748 - val_loss: 0.1612 - val_accuracy: 0.9510\n",
      "Epoch 148/200\n",
      "177/177 [==============================] - 3s 18ms/step - loss: 0.0617 - accuracy: 0.9760 - val_loss: 0.2296 - val_accuracy: 0.9345\n",
      "Epoch 149/200\n",
      "177/177 [==============================] - 3s 18ms/step - loss: 0.1226 - accuracy: 0.9615 - val_loss: 0.1524 - val_accuracy: 0.9505\n",
      "Epoch 150/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0850 - accuracy: 0.9716 - val_loss: 0.2332 - val_accuracy: 0.9367\n",
      "Epoch 151/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0835 - accuracy: 0.9684 - val_loss: 0.1509 - val_accuracy: 0.9510\n",
      "Epoch 152/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.1387 - accuracy: 0.9585 - val_loss: 0.1823 - val_accuracy: 0.9431\n",
      "Epoch 153/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0779 - accuracy: 0.9736 - val_loss: 0.1567 - val_accuracy: 0.9537\n",
      "Epoch 154/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0621 - accuracy: 0.9778 - val_loss: 0.1597 - val_accuracy: 0.9447\n",
      "Epoch 155/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0707 - accuracy: 0.9752 - val_loss: 0.1721 - val_accuracy: 0.9526\n",
      "Epoch 156/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.0661 - accuracy: 0.9760 - val_loss: 0.1848 - val_accuracy: 0.9473\n",
      "Epoch 157/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0504 - accuracy: 0.9828 - val_loss: 0.1844 - val_accuracy: 0.9510\n",
      "Epoch 158/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.0884 - accuracy: 0.9673 - val_loss: 0.2102 - val_accuracy: 0.9372\n",
      "Epoch 159/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0755 - accuracy: 0.9732 - val_loss: 0.1824 - val_accuracy: 0.9489\n",
      "Epoch 160/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.0531 - accuracy: 0.9794 - val_loss: 0.1798 - val_accuracy: 0.9494\n",
      "Epoch 161/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.0916 - accuracy: 0.9657 - val_loss: 0.1628 - val_accuracy: 0.9436\n",
      "Epoch 162/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0561 - accuracy: 0.9805 - val_loss: 0.1784 - val_accuracy: 0.9361\n",
      "Epoch 163/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.0670 - accuracy: 0.9755 - val_loss: 0.2049 - val_accuracy: 0.9409\n",
      "Epoch 164/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0552 - accuracy: 0.9792 - val_loss: 0.1591 - val_accuracy: 0.9521\n",
      "Epoch 165/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.0785 - accuracy: 0.9725 - val_loss: 0.2246 - val_accuracy: 0.9361\n",
      "Epoch 166/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0769 - accuracy: 0.9746 - val_loss: 0.1464 - val_accuracy: 0.9580\n",
      "Epoch 167/200\n",
      "177/177 [==============================] - 3s 19ms/step - loss: 0.0485 - accuracy: 0.9830 - val_loss: 0.1382 - val_accuracy: 0.9574\n",
      "Epoch 168/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0581 - accuracy: 0.9791 - val_loss: 0.1340 - val_accuracy: 0.9569\n",
      "Epoch 169/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.0418 - accuracy: 0.9838 - val_loss: 0.1783 - val_accuracy: 0.9500\n",
      "Epoch 170/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0695 - accuracy: 0.9732 - val_loss: 0.1844 - val_accuracy: 0.9388\n",
      "Epoch 171/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0651 - accuracy: 0.9769 - val_loss: 0.1366 - val_accuracy: 0.9564\n",
      "Epoch 172/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.0410 - accuracy: 0.9849 - val_loss: 0.1484 - val_accuracy: 0.9617\n",
      "Epoch 173/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.0548 - accuracy: 0.9791 - val_loss: 0.1952 - val_accuracy: 0.9473\n",
      "Epoch 174/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.0523 - accuracy: 0.9808 - val_loss: 0.2408 - val_accuracy: 0.9383\n",
      "Epoch 175/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0533 - accuracy: 0.9814 - val_loss: 0.2604 - val_accuracy: 0.9420\n",
      "Epoch 176/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0561 - accuracy: 0.9792 - val_loss: 0.2288 - val_accuracy: 0.9351\n",
      "Epoch 177/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.0479 - accuracy: 0.9830 - val_loss: 0.1811 - val_accuracy: 0.9510\n",
      "Epoch 178/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.0526 - accuracy: 0.9826 - val_loss: 0.1477 - val_accuracy: 0.9569\n",
      "Epoch 179/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.0655 - accuracy: 0.9766 - val_loss: 0.1758 - val_accuracy: 0.9537\n",
      "Epoch 180/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.0676 - accuracy: 0.9757 - val_loss: 0.1701 - val_accuracy: 0.9489\n",
      "Epoch 181/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.0600 - accuracy: 0.9803 - val_loss: 0.1650 - val_accuracy: 0.9526\n",
      "Epoch 182/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0349 - accuracy: 0.9869 - val_loss: 0.1750 - val_accuracy: 0.9590\n",
      "Epoch 183/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0553 - accuracy: 0.9817 - val_loss: 0.1639 - val_accuracy: 0.9574\n",
      "Epoch 184/200\n",
      "177/177 [==============================] - 3s 18ms/step - loss: 0.0622 - accuracy: 0.9775 - val_loss: 0.2000 - val_accuracy: 0.9393\n",
      "Epoch 185/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0491 - accuracy: 0.9830 - val_loss: 0.2099 - val_accuracy: 0.9388\n",
      "Epoch 186/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0403 - accuracy: 0.9862 - val_loss: 0.1697 - val_accuracy: 0.9489\n",
      "Epoch 187/200\n",
      "177/177 [==============================] - 3s 18ms/step - loss: 0.0435 - accuracy: 0.9828 - val_loss: 0.1790 - val_accuracy: 0.9526\n",
      "Epoch 188/200\n",
      "177/177 [==============================] - 3s 18ms/step - loss: 0.0502 - accuracy: 0.9837 - val_loss: 0.1934 - val_accuracy: 0.9500\n",
      "Epoch 189/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0777 - accuracy: 0.9730 - val_loss: 0.1685 - val_accuracy: 0.9537\n",
      "Epoch 190/200\n",
      "177/177 [==============================] - 3s 18ms/step - loss: 0.0346 - accuracy: 0.9870 - val_loss: 0.1652 - val_accuracy: 0.9574\n",
      "Epoch 191/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0400 - accuracy: 0.9860 - val_loss: 0.1923 - val_accuracy: 0.9516\n",
      "Epoch 192/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0472 - accuracy: 0.9824 - val_loss: 0.2266 - val_accuracy: 0.9484\n",
      "Epoch 193/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0528 - accuracy: 0.9821 - val_loss: 0.1866 - val_accuracy: 0.9516\n",
      "Epoch 194/200\n",
      "177/177 [==============================] - 3s 18ms/step - loss: 0.0292 - accuracy: 0.9894 - val_loss: 0.1709 - val_accuracy: 0.9548\n",
      "Epoch 195/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0701 - accuracy: 0.9757 - val_loss: 0.2113 - val_accuracy: 0.9409\n",
      "Epoch 196/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.0437 - accuracy: 0.9831 - val_loss: 0.1775 - val_accuracy: 0.9548\n",
      "Epoch 197/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.0806 - accuracy: 0.9709 - val_loss: 0.1756 - val_accuracy: 0.9473\n",
      "Epoch 198/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0494 - accuracy: 0.9835 - val_loss: 0.1702 - val_accuracy: 0.9590\n",
      "Epoch 199/200\n",
      "177/177 [==============================] - 3s 16ms/step - loss: 0.0350 - accuracy: 0.9881 - val_loss: 0.2514 - val_accuracy: 0.9420\n",
      "Epoch 200/200\n",
      "177/177 [==============================] - 3s 17ms/step - loss: 0.0450 - accuracy: 0.9851 - val_loss: 0.1547 - val_accuracy: 0.9580\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) # Compile the model using the adam optimizer\n",
    "history = model.fit(train, epochs=200, batch_size=32, validation_data=val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59/59 [==============================] - 0s 5ms/step - loss: 0.1556 - accuracy: 0.9643\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1556152105331421, 0.9643427133560181]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model on the test data\n",
    "model.evaluate(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "96% accuracy, not bad! Let's look at our loss across the epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f9fa4db3cd0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = pd.Series(history.history['loss'])\n",
    "losses.plot.line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like at the end it was bouncing around a lot. To improve the performance we may need to dynamically change the learning rate of the optimizer as we train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pull out the labels from the test set so that we can generate a confusion matrix\n",
    "labels = []\n",
    "for items in test:\n",
    "    for item in items[1]:\n",
    "        labels.append(item.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[877,  65],\n",
       "       [  2, 935]], dtype=int32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.confusion_matrix(labels, model.predict_classes(test),).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is also a good model, but the accuracy of the random forest classifer was 98 percent which is a little bit better. However, the deep-learning model does better with reducing the false negatives (2), which may be of more importance when predicting stroke compared to doing better at false positives (65)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial random forest classifier was the best classifier we built. The deep learning did very well as well, but it was not the best in terms of raw performance. However, if false negatives are a clinical concern then it may be advantageous to select the deep-learning model. The deep-learning model is only a simple model and can be improved further by adding more layers and tuning somee hyper-paramters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
